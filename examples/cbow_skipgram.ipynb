{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf163ff7-1bbc-41ee-bacd-f31e784eea4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acddb2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x117c99fd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "247bd357-3bfb-4438-92aa-d955d027d2c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "max_norm = 1\n",
    "min_freq = 50\n",
    "num_words = 4\n",
    "max_sequence_length = 256\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 96\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "158a765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, stoi, default_index):\n",
    "        self.stoi = stoi\n",
    "        self.itos = {i: t for t, i in stoi.items()}\n",
    "        self.default_index = default_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return self.stoi.get(token, self.default_index)\n",
    "\n",
    "    def __call__(self, tokens):\n",
    "        return self.lookup_indices(tokens)\n",
    "\n",
    "    def lookup_indices(self, tokens):\n",
    "        return [self[token] for token in tokens]\n",
    "\n",
    "    def lookup_token(self, index):\n",
    "        return self.itos[index]\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.lookup_indices(tokens)\n",
    "\n",
    "    def get_stoi(self):\n",
    "        return self.stoi\n",
    "\n",
    "    def set_default_index(self, index):\n",
    "        self.default_index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31f51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_from_iterator(token_iter, specials=[\"<unk>\"], min_freq=1):\n",
    "    counter = Counter()\n",
    "    for tokens in token_iter:\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # Filter by min_freq and sort by frequency descending, then alphabetically\n",
    "    word_freq = sorted([(w, f) for w, f in counter.items() if f >= min_freq], key=lambda x: (-x[1], x[0]))\n",
    "    words = specials + [w for w, _ in word_freq]\n",
    "    stoi = {word: idx for idx, word in enumerate(words)}\n",
    "    default_index = stoi[\"<unk>\"]\n",
    "    return Vocab(stoi, default_index)\n",
    "\n",
    "\n",
    "def get_tokenizer(tokenizer_name, language=\"en\"):\n",
    "    if tokenizer_name != \"basic_english\":\n",
    "        raise ValueError(f\"Unsupported tokenizer: {tokenizer_name}\")\n",
    "    def basic_english_tokenize(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"([.,!?;])\", r\" \\1 \", text)\n",
    "        text = re.sub(r\"[^a-zA-Z.,!?;]+\", r\" \", text)\n",
    "        return text.split()\n",
    "    return basic_english_tokenize\n",
    "\n",
    "\n",
    "def build_vocab(data_iter, tokenizer):\n",
    "    vocab = build_vocab_from_iterator(map(tokenizer, data_iter), specials=[\"<unk>\"], min_freq=min_freq)\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def collate_cbow(batch, text_pipeline):\n",
    "    inputs, outputs = [], []\n",
    "    for text in batch:\n",
    "        token_ids = text_pipeline(text)\n",
    "        if len(token_ids) < num_words * 2 + 1:\n",
    "            continue\n",
    "        token_ids = token_ids[:max_sequence_length]\n",
    "        for i in range(len(token_ids) - num_words * 2):\n",
    "            sequence = token_ids[i : num_words * 2 + i + 1]\n",
    "            outputs.append(sequence.pop(num_words))\n",
    "            inputs.append(sequence)\n",
    "    return torch.tensor(inputs, dtype=torch.long), torch.tensor(outputs, dtype=torch.long)\n",
    "\n",
    "\n",
    "def get_dataloader_and_vocab(ds_type, batch_size, shuffle=True, vocab=None):\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=ds_type)\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "    def data_iter():\n",
    "        for example in dataset:\n",
    "            yield example[\"text\"]\n",
    "\n",
    "    if not vocab:\n",
    "        vocab = build_vocab(data_iter(), tokenizer)\n",
    "\n",
    "    text_pipeline = lambda x: vocab(tokenizer(x))  # vocab(tokens) works via forward/lookup_indices\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        texts = [example[\"text\"] for example in batch]\n",
    "        return collate_cbow(texts, text_pipeline)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "    return dataloader, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58f216c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3867\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, vocab = get_dataloader_and_vocab(\"train\", batch_size)\n",
    "valid_dataloader, _ = get_dataloader_and_vocab(\"validation\", batch_size, vocab=vocab)\n",
    "vocab_size = len(vocab.get_stoi())\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "train_steps, valid_steps = len(train_dataloader), len(valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f728ae6d-cd10-460c-9cd8-2dbb6f09e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim, max_norm=max_norm)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x.mean(axis=1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "223c4934",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25e10701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.35: 100%|██████████| 383/383 [00:22<00:00, 16.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 5.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 4.86: 100%|██████████| 383/383 [00:20<00:00, 18.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 5.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.04: 100%|██████████| 383/383 [00:21<00:00, 18.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 5.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 4.95: 100%|██████████| 383/383 [00:22<00:00, 17.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 4.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 4.97: 100%|██████████| 383/383 [00:22<00:00, 16.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 4.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 4.70: 100%|██████████| 383/383 [00:22<00:00, 16.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 4.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.00: 100%|██████████| 383/383 [00:21<00:00, 18.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 4.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 4.94: 100%|██████████| 383/383 [00:21<00:00, 17.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 4.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 4.89: 100%|██████████| 383/383 [00:21<00:00, 18.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 4.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 4.77: 100%|██████████| 383/383 [00:21<00:00, 17.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 4.78\n"
     ]
    }
   ],
   "source": [
    "for _ in range(epochs):\n",
    "    model.train()\n",
    "    for i, (x, y) in (t := tqdm(enumerate(train_dataloader), total=train_steps)):\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t.set_description(f\"train loss {loss.item():.2f}\")\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    for i, (x, y) in enumerate(valid_dataloader):\n",
    "        loss = criterion(model(x), y)\n",
    "        total += loss.item()\n",
    "    print(f\"validation loss {(total/valid_steps):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57d02548-4dcc-4e5d-9500-ac0347ac1de8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3867, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = next(model.parameters()).detach().numpy()\n",
    "norms = ((embeddings ** 2).sum(axis=1) ** 0.5).reshape(-1, 1)\n",
    "embeddings_norm = embeddings / norms\n",
    "embeddings_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4438cfd5-f9f5-4ef5-bb82-86f7dc00e96e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wife: 0.863\n",
      "brother: 0.852\n",
      "daughter: 0.821\n",
      "mother: 0.801\n",
      "son: 0.733\n",
      "friend: 0.711\n",
      "husband: 0.702\n",
      "opponent: 0.690\n",
      "successor: 0.656\n",
      "death: 0.638\n"
     ]
    }
   ],
   "source": [
    "def get_similar(word, n=10):\n",
    "    word_id = vocab[word]\n",
    "    if word_id == 0:\n",
    "        print(\"out of vocabulary word\")\n",
    "        return {}\n",
    "    word_vec = embeddings_norm[word_id].flatten()\n",
    "    dists = np.matmul(embeddings_norm, word_vec).flatten()\n",
    "    top_ids = np.argsort(-dists)[1:n+1]\n",
    "    top_dict = {}\n",
    "    for sim_word_id in top_ids:\n",
    "        sim_word = vocab.lookup_token(sim_word_id)\n",
    "        top_dict[sim_word] = dists[sim_word_id]\n",
    "    return top_dict\n",
    "\n",
    "\n",
    "for word, score in get_similar(\"father\").items():\n",
    "    print(f\"{word}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bfae95c-1409-4c85-ad36-357657fa44e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king: 0.714\n",
      "queen: 0.655\n",
      "lord: 0.607\n",
      "ambassador: 0.595\n",
      "representative: 0.595\n",
      "church: 0.551\n",
      "mother: 0.550\n",
      "palace: 0.549\n",
      "nation: 0.539\n",
      "henry: 0.534\n"
     ]
    }
   ],
   "source": [
    "emb = embeddings[vocab[\"king\"]] - embeddings[vocab[\"man\"]] + embeddings[vocab[\"woman\"]]\n",
    "norm = (emb ** 2).sum() ** 0.5\n",
    "emb_norm = (emb / norm).flatten()\n",
    "dists = np.matmul(embeddings_norm, emb_norm).flatten()\n",
    "word_ids = np.argsort(-dists)[:10]\n",
    "\n",
    "for word_id in word_ids:\n",
    "    print(f\"{vocab.lookup_token(word_id)}: {dists[word_id]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64789d8b-0919-4898-bea7-f5da4d2caa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim, max_norm=max_norm)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a979d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_skipgram(batch, text_pipeline):\n",
    "    targets, contexts = [], []\n",
    "    for text in batch:\n",
    "        token_ids = text_pipeline(text)\n",
    "        if len(token_ids) < 2:\n",
    "            continue\n",
    "        token_ids = token_ids[:max_sequence_length]\n",
    "        for i in range(len(token_ids)):\n",
    "            for offset in range(1, num_words + 1):\n",
    "                if i - offset >= 0:\n",
    "                    targets.append(token_ids[i])\n",
    "                    contexts.append(token_ids[i - offset])\n",
    "                if i + offset < len(token_ids):\n",
    "                    targets.append(token_ids[i])\n",
    "                    contexts.append(token_ids[i + offset])\n",
    "\n",
    "    if not targets:\n",
    "        return torch.empty(0, dtype=torch.long), torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    return torch.tensor(targets, dtype=torch.long), torch.tensor(contexts, dtype=torch.long)\n",
    "\n",
    "\n",
    "def get_dataloader_and_vocab(ds_type, batch_size, shuffle=True, vocab=None):\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=ds_type)\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "    def data_iter():\n",
    "        for example in dataset:\n",
    "            yield example[\"text\"]\n",
    "\n",
    "    if not vocab:\n",
    "        vocab = build_vocab(data_iter(), tokenizer)\n",
    "\n",
    "    text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        texts = [example[\"text\"] for example in batch]\n",
    "        return collate_skipgram(texts, text_pipeline)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "    return dataloader, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0a7dae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3867\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, vocab = get_dataloader_and_vocab(\"train\", batch_size)\n",
    "valid_dataloader, _ = get_dataloader_and_vocab(\"validation\", batch_size)\n",
    "vocab_size = len(vocab.get_stoi())\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "train_steps, valid_steps = len(train_dataloader), len(valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93283167",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGram(vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5d0d8a1-5bbf-4084-8efb-b866b50faeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.40: 100%|██████████| 383/383 [02:21<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.44: 100%|██████████| 383/383 [02:11<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.50: 100%|██████████| 383/383 [02:09<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.34: 100%|██████████| 383/383 [02:06<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.45: 100%|██████████| 383/383 [02:11<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.40: 100%|██████████| 383/383 [02:19<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.41: 100%|██████████| 383/383 [02:18<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.32: 100%|██████████| 383/383 [02:12<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.24: 100%|██████████| 383/383 [02:04<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.38: 100%|██████████| 383/383 [02:04<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.91\n"
     ]
    }
   ],
   "source": [
    "for _ in range(epochs):\n",
    "    model.train()\n",
    "    for i, (x, y) in (t := tqdm(enumerate(train_dataloader), total=train_steps)):\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t.set_description(f\"train loss {loss.item():.2f}\")\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    for i, (x, y) in enumerate(valid_dataloader):\n",
    "        loss = criterion(model(x), y)\n",
    "        total += loss.item()\n",
    "    print(f\"validation loss {(total/valid_steps):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4fb648d-efb2-4270-88e8-3a44d9948a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3867, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = next(model.parameters()).detach().numpy()\n",
    "norms = ((embeddings ** 2).sum(axis=1) ** 0.5).reshape(-1, 1)\n",
    "embeddings_norm = embeddings / norms\n",
    "embeddings_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77648e70-1055-4dee-be5f-93c168c1522b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mother: 0.739\n",
      "brother: 0.712\n",
      "daughter: 0.699\n",
      "son: 0.682\n",
      "wife: 0.675\n",
      "parents: 0.669\n",
      "friend: 0.663\n",
      "husband: 0.635\n",
      "pitman: 0.621\n",
      "marriage: 0.616\n"
     ]
    }
   ],
   "source": [
    "for word, score in get_similar(\"father\").items():\n",
    "    print(f\"{word}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e94d5394-6f73-42eb-99a8-0f5d6a5ea1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king: 0.685\n",
      "queen: 0.607\n",
      "woman: 0.597\n",
      "daughter: 0.593\n",
      "henry: 0.584\n",
      "calvert: 0.579\n",
      "edward: 0.555\n",
      "reign: 0.555\n",
      "lord: 0.554\n",
      "elizabeth: 0.537\n"
     ]
    }
   ],
   "source": [
    "emb = embeddings[vocab[\"king\"]] - embeddings[vocab[\"man\"]] + embeddings[vocab[\"woman\"]]\n",
    "norm = (emb ** 2).sum() ** 0.5\n",
    "emb_norm = (emb / norm).flatten()\n",
    "dists = np.matmul(embeddings_norm, emb_norm).flatten()\n",
    "word_ids = np.argsort(-dists)[:10]\n",
    "\n",
    "for word_id in word_ids:\n",
    "    print(f\"{vocab.lookup_token(word_id)}: {dists[word_id]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83953a8-4017-4f23-b38f-25c87d0e7c49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fromthetensor (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
