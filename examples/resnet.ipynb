{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax import nnx\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-01-18 19:43:11,864:jax._src.xla_bridge:1000: Platform 'METAL' is experimental and not all JAX functionality may be correctly supported!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1737258191.864434 24777688 mps_client.cc:510] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported!\n",
      "I0000 00:00:1737258191.872177 24777688 service.cc:145] XLA service 0x131a4ecb0 initialized for platform METAL (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1737258191.872191 24777688 service.cc:153]   StreamExecutor device (0): Metal, <undefined>\n",
      "I0000 00:00:1737258191.873634 24777688 mps_client.cc:406] Using Simple allocator.\n",
      "I0000 00:00:1737258191.873651 24777688 mps_client.cc:384] XLA backend will use up to 11452858368 bytes on device 0 for SimpleAllocator.\n"
     ]
    }
   ],
   "source": [
    "rngs = nnx.Rngs(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x):\n",
    "    x = x.reshape(-1, 3, 32, 32)\n",
    "    x = [[Image.fromarray(z).resize((224, 224)) for z in y] for y in x]\n",
    "    x = np.stack([np.stack([np.asarray(z) for z in y], axis=0) for y in x], axis=0)\n",
    "    x = x.reshape(-1, 224, 224, 3)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cifar10\")\n",
    "\n",
    "X_train = np.array([np.array(image) for image in dataset[\"train\"][\"img\"]]) / 255.0 - 0.5\n",
    "Y_train = np.array(dataset[\"train\"][\"label\"], dtype=np.int32)\n",
    "\n",
    "X_test = np.array([np.array(image) for image in dataset[\"test\"][\"img\"]]) / 255.0 - 0.5\n",
    "Y_test = np.array(dataset[\"test\"][\"label\"], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock:\n",
    "    def __init__(self, inplanes, planes, strides=(1, 1), downsample=None):\n",
    "        self.conv1 = nnx.Conv(inplanes, planes, kernel_size=(3, 3), strides=strides, padding=(1, 1), use_bias=False, rngs=rngs)\n",
    "        self.bn1 = nnx.BatchNorm(num_features=planes, rngs=rngs)\n",
    "        self.conv2 = nnx.Conv(planes, planes, kernel_size=(3, 3), strides=(1, 1), padding=(1, 1), use_bias=False, rngs=rngs)\n",
    "        self.bn2 = nnx.BatchNorm(num_features=planes, rngs=rngs)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = nnx.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        out = nnx.relu(out + x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nnx.Module):\n",
    "    def __init__(self):\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nnx.Conv(3, 64, kernel_size=(7, 7), strides=(2, 2), padding=(3, 3), use_bias=False, rngs=rngs)\n",
    "        self.bn1 = nnx.BatchNorm(num_features=64, rngs=rngs)\n",
    "        self.max_pool = partial(nnx.max_pool, window_shape=(3, 3), strides=(2, 2), padding=((1, 1), (1, 1)))\n",
    "        self.layer1 = self._make_layer(64, 3, strides=(1, 1))\n",
    "        self.layer2 = self._make_layer(128, 4, strides=(2, 2))\n",
    "        self.layer3 = self._make_layer(256, 6, strides=(2, 2))\n",
    "        self.layer4 = self._make_layer(512, 3, strides=(2, 2))\n",
    "        self.avg_pool = partial(nnx.avg_pool, window_shape=(7, 7), strides=(1, 1))\n",
    "        self.linear = nnx.Linear(512, 10, rngs=rngs)\n",
    "\n",
    "    def _make_layer(self, planes, blocks, strides=(1, 1)):\n",
    "        downsample = None\n",
    "        if strides != (1, 1) or self.inplanes != planes:\n",
    "            downsample = nnx.Sequential(\n",
    "                nnx.Conv(self.inplanes, planes, kernel_size=(1, 1), strides=strides, use_bias=False, rngs=rngs),\n",
    "                nnx.BatchNorm(num_features=planes, rngs=rngs),\n",
    "            )\n",
    "        layers = [BasicBlock(self.inplanes, planes, strides=strides, downsample=downsample)]\n",
    "        self.inplanes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock(self.inplanes, planes))\n",
    "        return nnx.Sequential(*layers)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = nnx.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.max_pool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.19926558  0.10369247 -0.41314387 -0.70792234 -0.1057477   1.4667993\n",
      "  -0.0979929  -0.6685898  -1.5630112   0.17691338]]\n"
     ]
    }
   ],
   "source": [
    "model = ResNet()\n",
    "y = model(jnp.ones((1, 224, 224, 3)))\n",
    "nnx.display(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "\n",
    "optimizer = nnx.Optimizer(model, optax.sgd(learning_rate, momentum))\n",
    "metrics = nnx.MultiMetric(accuracy=nnx.metrics.Accuracy(), loss=nnx.metrics.Average(\"loss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, images, labels):\n",
    "    logits = model(images)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=labels).mean()\n",
    "    return loss, logits\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, metrics, images, labels):\n",
    "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(model, images, labels)\n",
    "    metrics.update(loss=loss, logits=logits, labels=labels)\n",
    "    optimizer.update(grads)\n",
    "\n",
    "@nnx.jit\n",
    "def eval_step(model, metrics, images, labels):\n",
    "    loss, logits = loss_fn(model, images, labels)\n",
    "    metrics.update(loss=loss, logits=logits, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnexpectedTracerError",
     "evalue": "Encountered an unexpected tracer. A function transformed by JAX had a side effect, allowing for a reference to an intermediate value with type float32[64] wrapped in a JVPTracer to escape the scope of the transformation.\nJAX transformations require that functions explicitly return their outputs, and disallow saving intermediate values to global state.\nTo catch the leak earlier, try setting the environment variable JAX_CHECK_TRACER_LEAKS or using the `jax.checking_leaks` context manager.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.UnexpectedTracerError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedTracerError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m     images \u001b[38;5;241m=\u001b[39m transform(X_test[batch_size\u001b[38;5;241m*\u001b[39mtest_step:batch_size\u001b[38;5;241m*\u001b[39m(test_step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)])\n\u001b[1;32m     21\u001b[0m     labels \u001b[38;5;241m=\u001b[39m Y_test[batch_size\u001b[38;5;241m*\u001b[39mtest_step:batch_size\u001b[38;5;241m*\u001b[39m(test_step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m---> 22\u001b[0m     \u001b[43meval_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric, value \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mcompute()\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     25\u001b[0m     metrics_history[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(value)\n",
      "File \u001b[0;32m~/repos/fromthetensor/env/lib/python3.12/site-packages/flax/nnx/graph.py:1081\u001b[0m, in \u001b[0;36mUpdateContextManager.__call__.<locals>.update_context_manager_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_context_manager_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1080\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m-> 1081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/fromthetensor/env/lib/python3.12/site-packages/flax/nnx/transforms/compilation.py:345\u001b[0m, in \u001b[0;36mjit.<locals>.jit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fun)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;129m@graph\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_context(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    338\u001b[0m   pure_args, pure_kwargs \u001b[38;5;241m=\u001b[39m extract\u001b[38;5;241m.\u001b[39mto_tree(\n\u001b[1;32m    339\u001b[0m     (args, kwargs),\n\u001b[1;32m    340\u001b[0m     prefix\u001b[38;5;241m=\u001b[39m(in_shardings, kwarg_shardings),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m     ctxtag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjit\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    344\u001b[0m   )\n\u001b[0;32m--> 345\u001b[0m   pure_args_out, pure_kwargs_out, pure_out \u001b[38;5;241m=\u001b[39m \u001b[43mjitted_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpure_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpure_kwargs\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m   _args_out, _kwargs_out, out \u001b[38;5;241m=\u001b[39m extract\u001b[38;5;241m.\u001b[39mfrom_tree(\n\u001b[1;32m    349\u001b[0m     (pure_args_out, pure_kwargs_out, pure_out), ctxtag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjit\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    350\u001b[0m   )\n\u001b[1;32m    351\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/repos/fromthetensor/env/lib/python3.12/site-packages/jax/_src/core.py:924\u001b[0m, in \u001b[0;36mcheck_eval_args\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args:\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, Tracer):\n\u001b[0;32m--> 924\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m escaped_tracer_error(arg)\n",
      "\u001b[0;31mUnexpectedTracerError\u001b[0m: Encountered an unexpected tracer. A function transformed by JAX had a side effect, allowing for a reference to an intermediate value with type float32[64] wrapped in a JVPTracer to escape the scope of the transformation.\nJAX transformations require that functions explicitly return their outputs, and disallow saving intermediate values to global state.\nTo catch the leak earlier, try setting the environment variable JAX_CHECK_TRACER_LEAKS or using the `jax.checking_leaks` context manager.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.UnexpectedTracerError"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "eval_every = len(X_train)\n",
    "train_steps = len(X_train) // batch_size\n",
    "test_steps = len(X_test) // batch_size\n",
    "metrics_history = {\"train_loss\": [], \"train_accuracy\": [], \"test_loss\": [], \"test_accuracy\": []}\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step in range(train_steps):\n",
    "        sample = np.random.randint(0, len(X_train), size=batch_size)\n",
    "        images, labels = transform(X_train[sample]), Y_train[sample]\n",
    "        train_step(model, optimizer, metrics, images, labels)\n",
    "\n",
    "        if step > 0 and (step % eval_every == 0 or step == train_steps - 1):\n",
    "            for metric, value in metrics.compute().items():\n",
    "                metrics_history[f\"train_{metric}\"].append(value)\n",
    "            metrics.reset()\n",
    "\n",
    "            for test_step in range(test_steps):\n",
    "                images = transform(X_test[batch_size*test_step:batch_size*(test_step+1)])\n",
    "                labels = Y_test[batch_size*test_step:batch_size*(test_step+1)]\n",
    "                eval_step(model, metrics, images, labels)\n",
    "\n",
    "            for metric, value in metrics.compute().items():\n",
    "                metrics_history[f\"test_{metric}\"].append(value)\n",
    "            metrics.reset()\n",
    "\n",
    "            print(\n",
    "                f\"[train] epoch: {epoch}, step: {step}, \"\n",
    "                f\"loss: {metrics_history['train_loss'][-1]:.4f}, \"\n",
    "                f\"accuracy: {metrics_history['train_accuracy'][-1] * 100:.2f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"[test] epoch: {epoch}, step: {step}, \"\n",
    "                f\"loss: {metrics_history['test_loss'][-1]:.4f}, \"\n",
    "                f\"accuracy: {metrics_history['test_accuracy'][-1] * 100:.2f}\"\n",
    "            )\n",
    "    learning_rate *= 0.8\n",
    "    optimizer = nnx.Optimizer(model, optax.sgd(learning_rate, momentum))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
